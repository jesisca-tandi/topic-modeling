{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT5153 TOPICS IN BUSINESS ANALYTICS - Group Project \n",
    "# Understanding Customer in the Airline Industry on Social Media\n",
    "# Group 7 - Team NLP\n",
    "Zhang Kang En\tA0186050L \n",
    "\n",
    "Chee Wai Kin Simon\tA0186100U\n",
    "\n",
    "Toh Jing Xiang Joshua\tA0186795E\n",
    "\n",
    "Jesisca Tandi\tA0185994E\n",
    "\n",
    "Su Yixi Jessie\tA0054353L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob, random, spacy, re, inspect, pickle, os\n",
    "from textblob import TextBlob\n",
    "from random import sample\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, make_union\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, Activation, Flatten, Conv1D, GlobalMaxPooling1D, Dropout, Concatenate\n",
    "from keras.utils import to_categorical, np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "from IPython.core.display import display, HTML\n",
    "from gensim.models import KeyedVectors\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotated data  3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "others                1664\n",
       "t_flight_bookings     318 \n",
       "t_customer_service    296 \n",
       "t_flight_delays       288 \n",
       "t_in-flight           177 \n",
       "t_luggages            138 \n",
       "t_club                64  \n",
       "t_seatings            55  \n",
       "Name: topic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "info          1566\n",
       "complaint     775 \n",
       "enquiry       384 \n",
       "compliment    275 \n",
       "Name: feedback, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "neutral     1734\n",
       "negative    910 \n",
       "positive    356 \n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load manually labelled data\n",
    "# path = 'labelled tweets/' # Path to the annotated data\n",
    "path = 'annotated/' # Path to the annotated data\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "li = []\n",
    "\n",
    "y_variables = ['t_luggages','t_flight_delays', 't_flight_bookings', 't_club', 't_customer_service','t_in-flight','t_seatings',\n",
    "               'f_enquiry','f_compliment', 'f_complaint', 'f_info', \n",
    "               's_positive', 's_negative', 's_neutral']\n",
    "allvars =  ['text', 'emoji'] + y_variables\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df = df[allvars]\n",
    "    li.append(df)\n",
    "\n",
    "labelled_tweets = pd.concat(li, axis=0, ignore_index=True)\n",
    "labelled_tweets = labelled_tweets.fillna(0)\n",
    "print('Total annotated data ',labelled_tweets.shape[0])\n",
    "\n",
    "\n",
    "# convert feedback to onehot encoding, if not classified, group into info\n",
    "labelled_tweets['feedback'] = np.where(labelled_tweets['f_enquiry'] == 1, 'enquiry', \n",
    "                                       np.where(labelled_tweets['f_compliment'] == 1, 'compliment', \n",
    "                                                np.where(labelled_tweets['f_complaint'] == 1, 'complaint', \n",
    "                                                         np.where(labelled_tweets['f_info'] == 1, 'info', 'info'))))\n",
    "feedbackLabels = ['complaint','compliment', 'enquiry', 'info']\n",
    "\n",
    "\n",
    "# convert topic to onehot encoding, if not classified, group into others\n",
    "labelled_tweets['topic'] = np.where(\n",
    "    labelled_tweets['t_luggages'] == 1, 't_luggages', \n",
    "    np.where(labelled_tweets['t_flight_delays'] == 1, 't_flight_delays', \n",
    "             np.where(labelled_tweets['t_flight_bookings'] == 1, 't_flight_bookings', \n",
    "                      np.where(labelled_tweets['t_club'] == 1, 't_club', \n",
    "                               np.where(labelled_tweets['t_customer_service'] == 1, 't_customer_service', \n",
    "                                        np.where(labelled_tweets['t_in-flight'] == 1, 't_in-flight', \n",
    "                                                 np.where(labelled_tweets['t_seatings'] == 1, 't_seatings', 'others')))))))\n",
    "topicLabels = ['others', 't_club', 't_customer_service', 't_flight_bookings', 't_flight_delays', 't_in-flight', 't_luggages', 't_seatings']\n",
    "\n",
    "# convert sentiment to onehot encoding, if not classified, group into others\n",
    "labelled_tweets['sentiment'] = np.where(labelled_tweets['s_positive'] == 1, 'positive',\n",
    "                                        np.where(labelled_tweets['s_negative'] == 1, 'negative',\n",
    "                                                 np.where(labelled_tweets['s_neutral'] == 1, 'neutral', 'neutral')))\n",
    "sentimentLabels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "\n",
    "display(pd.Series(labelled_tweets['topic']).value_counts())\n",
    "display(pd.Series(labelled_tweets['feedback']).value_counts())\n",
    "display(pd.Series(labelled_tweets['sentiment']).value_counts())\n",
    "\n",
    "# Folder to save the best model\n",
    "outFolder = 'bestModels'\n",
    "if not os.path.exists(outFolder):\n",
    "    os.makedirs(outFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train size = 2400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "others                1331\n",
       "t_flight_bookings     255 \n",
       "t_customer_service    237 \n",
       "t_flight_delays       230 \n",
       "t_in-flight           142 \n",
       "t_luggages            110 \n",
       "t_club                51  \n",
       "t_seatings            44  \n",
       "Name: topic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "info          1262\n",
       "complaint     609 \n",
       "enquiry       300 \n",
       "compliment    229 \n",
       "Name: feedback, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "neutral     1398\n",
       "negative    712 \n",
       "positive    290 \n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test size = 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "others                333\n",
       "t_flight_bookings     63 \n",
       "t_customer_service    59 \n",
       "t_flight_delays       58 \n",
       "t_in-flight           35 \n",
       "t_luggages            28 \n",
       "t_club                13 \n",
       "t_seatings            11 \n",
       "Name: topic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "info          304\n",
       "complaint     166\n",
       "enquiry       84 \n",
       "compliment    46 \n",
       "Name: feedback, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "neutral     336\n",
       "negative    198\n",
       "positive    66 \n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train-test split (80-20)\n",
    "X_train_withEmoji, X_test_withEmoji, labelled_tweets_Y_train, labelled_tweets_Y_test = train_test_split(labelled_tweets[['text','emoji']], labelled_tweets[['topic', 'feedback', 'sentiment']], \n",
    "                                                                                    test_size=0.2, stratify=labelled_tweets[['topic']], random_state=10)\n",
    "\n",
    "X_train = X_train_withEmoji.text\n",
    "X_test = X_test_withEmoji.text\n",
    "\n",
    "print('\\nTrain size =', X_train.shape[0])\n",
    "display(pd.Series(labelled_tweets_Y_train['topic']).value_counts())\n",
    "display(pd.Series(labelled_tweets_Y_train['feedback']).value_counts())\n",
    "display(pd.Series(labelled_tweets_Y_train['sentiment']).value_counts())\n",
    "\n",
    "print('\\nTest size =', X_test.shape[0])\n",
    "display(pd.Series(labelled_tweets_Y_test['topic']).value_counts())\n",
    "display(pd.Series(labelled_tweets_Y_test['feedback']).value_counts())\n",
    "display(pd.Series(labelled_tweets_Y_test['sentiment']).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters setting\n",
    "evaluationMethod = 'f1_macro'\n",
    "evaluationMethod_nn = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions for feature extraction and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# General function for dense NN\n",
    "def createNNModel(inpLayer, outLayer, n_neuron=[10], rate=0.8):\n",
    "\n",
    "    model = Sequential()\n",
    "    layerBefore = inpLayer\n",
    "    for i in n_neuron:\n",
    "        model.add(Dropout(rate = rate, input_shape=(layerBefore,)))\n",
    "        model.add(Dense(i, activation='relu'))\n",
    "        layerBefore = n_neuron\n",
    "    model.add(Dense(outLayer, activation=\"softmax\"))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[evaluationMethod_nn])\n",
    "    return model\n",
    "\n",
    "class kerasDenseNNWrapper():\n",
    "    \n",
    "    def __init__(self, dummyEnc=None, n_neuron=[10], rate=0.8, validation_split=0.2, epochs=10, batch_size=50, verbose=0):\n",
    "        frame = inspect.currentframe()\n",
    "        args, _, _, values = inspect.getargvalues(frame)\n",
    "        for i in args:\n",
    "            if str(i) != 'self':\n",
    "                setattr(self, i, values[i])\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        if not params:\n",
    "            return self\n",
    "        for key, value in params.items():\n",
    "            key, delim, sub_key = key.partition('__')\n",
    "            if delim:\n",
    "                setattr(self, sub_key, value)\n",
    "            else:\n",
    "                setattr(self, key, value)\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        if self.dummyEnc is not None:\n",
    "            y_enc = self.dummyEnc.transform(y)\n",
    "            y_dummy = np_utils.to_categorical(y_enc)\n",
    "            \n",
    "        inpLayer = X.shape[1]\n",
    "        outLayer = y_dummy.shape[1]\n",
    "        self.model = createNNModel(inpLayer, outLayer, n_neuron=self.n_neuron, rate=self.rate)\n",
    "        self.model.fit(X, y_dummy, validation_split=self.validation_split, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predicted_Y = self.model.predict(X)\n",
    "        predicted_class = self.dummyEnc.classes_[np.argmax(predicted_Y, axis=1)]\n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# General function for CNN\n",
    "def createCNNModel(sequenceLength, outLayer, vocabsize=1000, embeddingdim=20, filtersizes=[2,3,4,5], numfilters=3):\n",
    "\n",
    "    model_input = Input(shape=(sequenceLength,))\n",
    "    z = Embedding(vocabsize, embeddingdim, input_length=sequenceLength, name=\"embedding\")(model_input)\n",
    "    \n",
    "    # Convolutional Layer \n",
    "    conv_blocks = []\n",
    "    for sz in filtersizes:\n",
    "        conv = Conv1D(filters=numfilters,\n",
    "                      kernel_size=sz,\n",
    "                      padding=\"valid\",\n",
    "                      activation=\"relu\",\n",
    "                      strides=1)(z)\n",
    "        conv = GlobalMaxPooling1D()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    # Fully-connected Layer\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "    # This is multi-class classification problem, use softmax layer \n",
    "    model_output = Dense(outLayer, activation=\"softmax\")(z)\n",
    "    model = Model(model_input, model_output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[evaluationMethod_nn])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class kerasCNNWrapper():\n",
    "    \n",
    "    def __init__(self, dummyEnc=None, vocabsize=1000, embeddingdim=20, filtersizes=[2,3,4,5], numfilters=3, tokTrain=None, validation_split=0.2, epochs=10, batch_size=50, verbose=0):\n",
    "        frame = inspect.currentframe()\n",
    "        args, _, _, values = inspect.getargvalues(frame)\n",
    "        for i in args:\n",
    "            if str(i) != 'self':\n",
    "                setattr(self, i, values[i])\n",
    "                \n",
    "    def set_params(self, **params):\n",
    "        if not params:\n",
    "            return self\n",
    "        for key, value in params.items():\n",
    "            key, delim, sub_key = key.partition('__')\n",
    "            if delim:\n",
    "                setattr(self, sub_key, value)\n",
    "            else:\n",
    "                setattr(self, key, value)\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transformX(self, X, fitting=None, vocabsize=1000):\n",
    "\n",
    "        if fitting is True:\n",
    "            tok = Tokenizer(num_words=vocabsize)  ## here, we are set the max number of words to keep. The most common 7999 words will be kept\n",
    "            tok.fit_on_texts(X)\n",
    "            self.tok = tok\n",
    "            X_seq = self.tok.texts_to_sequences(X)\n",
    "            self.sequence_length = max([len(ele) for ele in X_seq]) \n",
    "        \n",
    "        # Convert string to index\n",
    "        X_seq = self.tok.texts_to_sequences(X)\n",
    "\n",
    "        # Padding\n",
    "        X_seq_padded = pad_sequences(X_seq, maxlen=self.sequence_length, padding='post')\n",
    "        \n",
    "        return X_seq_padded\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        if self.dummyEnc is not None:\n",
    "            y_enc = self.dummyEnc.transform(y)\n",
    "            y_dummy = np_utils.to_categorical(y_enc)\n",
    "            \n",
    "        X_trf = self.transformX(X, fitting=True, vocabsize=self.vocabsize)\n",
    "        inpLayer = X_trf.shape[1]\n",
    "        outLayer = y_dummy.shape[1]\n",
    "        \n",
    "        self.model = createCNNModel(inpLayer, outLayer, vocabsize=self.vocabsize, embeddingdim=self.embeddingdim, \n",
    "                                   filtersizes=self.filtersizes, numfilters=self.numfilters)\n",
    "        self.model.fit(X_trf, y_dummy, validation_split=self.validation_split, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X_trf = self.transformX(X, vocabsize=self.vocabsize)\n",
    "        predicted_Y = self.model.predict(X_trf)\n",
    "        predicted_class = self.dummyEnc.classes_[np.argmax(predicted_Y, axis=1)]\n",
    "        return predicted_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Feature engineering : convert emojis into text\n",
    "def processEmoji(emoji):\n",
    "    if emoji!=0:\n",
    "        processedEmoji = re.sub(r\"\\\\\\\\U\", \"U+\", \n",
    "                                re.sub(r\"\\\\\\\\U\", \"U+\", \n",
    "                                       re.sub(r\"\\\\\\\\U000\", \"U+\", \n",
    "                                              re.sub(r\"\\\\\\\\ufe0f\", \"\", \n",
    "                                                     str(emoji.encode('unicode-escape')).strip(\"b'\")).upper()))).split()\n",
    "    else: \n",
    "        processedEmoji = []\n",
    "    return processedEmoji\n",
    "\n",
    "emojiDictionary = pd.read_csv(\"full_emoji_list.csv\")\n",
    "\n",
    "def mapEmoji(emojiList):\n",
    "    if len(emojiList)>0:\n",
    "        emojiText = ''\n",
    "        for i in emojiList:\n",
    "            k = emojiDictionary[emojiDictionary.Code==i].Description.values\n",
    "            if len(k)>0:\n",
    "                emojiText = ' '.join([emojiText, k[0]])\n",
    "    else:\n",
    "        emojiText = ''\n",
    "    return emojiText\n",
    "\n",
    "def makeFeatureEmoji(X):\n",
    "    \n",
    "    output = X.emoji.apply(lambda x: mapEmoji(processEmoji(x)))\n",
    "    return output.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     2,
     10,
     15
    ]
   },
   "outputs": [],
   "source": [
    "# Feature engineering : get total number of words, total number of nouns, adj, adv, verb\n",
    "\n",
    "def countWord(data, typeOfPOS):\n",
    "    c = 0\n",
    "    for i in data:\n",
    "        if i[1] in typeOfPOS:\n",
    "            c += 1\n",
    "            \n",
    "    return c\n",
    "\n",
    "posDictionary = {'Noun': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "                'Adj': ['JJ', 'JJR', 'JJS'],\n",
    "                'Adv': ['RB', 'RBR', 'RBS'],\n",
    "                'Verb': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']}\n",
    "\n",
    "def combineFeatures(X):\n",
    "\n",
    "    manualFeatures = pd.DataFrame()\n",
    "    manualFeatures['textBlobOutput'] = X.text.apply(lambda x: TextBlob(x))\n",
    "    for k in posDictionary:\n",
    "        manualFeatures[k] = manualFeatures.textBlobOutput.apply(lambda x: countWord(x.tags, posDictionary[k]))\n",
    "    \n",
    "    manualFeatures['len'] = manualFeatures.textBlobOutput.apply(lambda x: len(x.words))\n",
    "    manualFeatures['polarity'] = manualFeatures.textBlobOutput.apply(lambda x: x.sentiment.polarity)\n",
    "    manualFeatures['subjectivity'] = manualFeatures.textBlobOutput.apply(lambda x: x.sentiment.subjectivity)\n",
    "    \n",
    "    manualFeatures = manualFeatures.drop('textBlobOutput', axis=1)\n",
    "    return manualFeatures.values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Feature engineering : Use GoogleNews pre-trained word vectors\n",
    "# wordEmbTrainedModel = KeyedVectors.load_word2vec_format('c:/Users/Kang En/Desktop/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "wordEmbTrainedModel = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "Nwordemb = 300\n",
    "def wordEmbFun_Convert(sentence):\n",
    "    myDict = {}\n",
    "    for t in ['NOUN', 'ADJ', 'VERB']:\n",
    "        myDict[t] = np.zeros((Nwordemb,1))\n",
    "        myDict['{}_N'.format(t)] = 0\n",
    "\n",
    "    word = nlp(sentence)\n",
    "    for i in word:\n",
    "        if (str(i.lemma_) in wordEmbTrainedModel) & (i.pos_ in myDict):\n",
    "            e = wordEmbTrainedModel[i.lemma_]\n",
    "            myDict[i.pos_] += e.reshape(e.shape[0],1)\n",
    "            myDict['{}_N'.format(i.pos_)] += 1\n",
    "\n",
    "    for t in ['NOUN', 'ADJ', 'VERB']:\n",
    "        if myDict['{}_N'.format(t)] != 0:\n",
    "            myDict[t] = myDict[t]/myDict['{}_N'.format(t)]\n",
    "\n",
    "    return np.concatenate((myDict['NOUN'], myDict['ADJ'], myDict['VERB']))\n",
    "\n",
    "def wordEmbFun(inp):\n",
    "    out = np.zeros((inp.shape[0], Nwordemb*3))\n",
    "    for i,j in enumerate(inp):\n",
    "        out[i,:] = wordEmbFun_Convert(j).flatten()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Function to print performance metrics\n",
    "def getPerformanceMetrics(y_test, pred_y_test, labelList=None):\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, pred_y_test)\n",
    "    print('Test accuracy\\t{:.03f}'.format(test_acc))\n",
    "    \n",
    "    f1_micro_score = f1_score(y_test, pred_y_test, average='micro')\n",
    "    print('F1 micro\\t{:.03f}'.format(f1_micro_score))\n",
    "    \n",
    "    f1_macro_score = f1_score(y_test, pred_y_test, average='macro')\n",
    "    print('F1 macro\\t{:.03f}'.format(f1_macro_score))\n",
    "    \n",
    "    conf_mat = confusion_matrix(y_test, pred_y_test, labels=labelList)\n",
    "    print('Confusion matrix\\n', conf_mat)\n",
    "    \n",
    "    return test_acc, f1_macro_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model 1] Word embedding + GLM\n",
    "Vectorizer method: use predefined word embedding vector from GoogleNews-vectors-negative300.bin, average over nouns, verbs, adjectives in each tweet.\n",
    "Model: GLM. Each pos will be a vector of 300, combined together becomes a vector with (900,) shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modelOne(X_train, y_train, X_test, y_test, tunedParms=None, saveModel=False, labelList=None):\n",
    "\n",
    "    print('='*5, \"\\tModel 1\\t\", '='*5)\n",
    "\n",
    "    # GLM + mean(NOUN, ADJ, VERB) word emb)\n",
    "    glm = LogisticRegression(class_weight='balanced', solver='lbfgs', max_iter=2000, multi_class='multinomial')\n",
    "    wordEmb = FunctionTransformer(wordEmbFun, validate=False)\n",
    "    pipe = make_pipeline(wordEmb, glm)\n",
    "\n",
    "    # create a grid of parameters to search\n",
    "    param_grid = {}\n",
    "    param_grid['logisticregression__C'] = [0.1, 0.3, 0.5, 0.7, 0.9] \n",
    "\n",
    "    if tunedParms is None:\n",
    "\n",
    "        # Start cross validation to tune hyperparameters\n",
    "        grid_t = GridSearchCV(pipe, param_grid, cv=5, scoring=evaluationMethod, return_train_score=True)\n",
    "        %time grid_t.fit(X_train, y_train)\n",
    "\n",
    "        # examine the best score\n",
    "        print(grid_t.best_score_)\n",
    "        print(grid_t.best_params_)\n",
    "\n",
    "        pred_y_test = grid_t.predict(X_test)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        pipe.set_params(**tunedParms)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        if saveModel:\n",
    "            pickle.dump(pipe, open(saveModel, 'wb'))\n",
    "        pred_y_test = pipe.predict(X_test)\n",
    "\n",
    "    test_acc, test_f1_macro = getPerformanceMetrics(y_test, pred_y_test, labelList=labelList)\n",
    "\n",
    "    if tunedParms is None:\n",
    "        return grid_t.best_params_\n",
    "            \n",
    "    return test_acc, test_f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model 2] BoW + Naive bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modelTwo(X_train, y_train, X_test, y_test, tunedParms=None, saveModel=False, labelList=None):\n",
    "\n",
    "    print('='*5, \"\\tModel 2\\t\", '='*5)\n",
    "\n",
    "    # import and instantiate Multinomial Naive Bayes (with the default parameters)\n",
    "    nb_t = MultinomialNB()\n",
    "    vect_t = CountVectorizer() \n",
    "    pipe = make_pipeline(vect_t, nb_t)\n",
    "\n",
    "    # create a grid of parameters to search\n",
    "    param_grid = {}\n",
    "    param_grid['countvectorizer__token_pattern'] = ['(?u)\\\\b\\\\w\\\\w+\\\\b', '\\\\b[^\\\\d\\\\W]+\\\\b', '([a-z ]+)']\n",
    "    param_grid['countvectorizer__stop_words'] = [\"english\"]\n",
    "    param_grid['countvectorizer__ngram_range'] = [(1, 1),(1,2)]\n",
    "    param_grid['countvectorizer__max_df'] = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    param_grid['countvectorizer__min_df'] = [3, 4, 5]\n",
    "    param_grid['multinomialnb__alpha'] = [0.2, 0.5, 0.7, 1]\n",
    "\n",
    "    if tunedParms is None:\n",
    "\n",
    "        # Start cross validation to tune hyperparameters\n",
    "        grid_t = GridSearchCV(pipe, param_grid, cv=5, scoring=evaluationMethod, return_train_score=True)\n",
    "        %time grid_t.fit(X_train, y_train)\n",
    "\n",
    "        # examine the best score\n",
    "        print(grid_t.best_score_)\n",
    "        print(grid_t.best_params_)\n",
    "\n",
    "        pred_y_test = grid_t.predict(X_test)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        pipe.set_params(**tunedParms)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        if saveModel:\n",
    "            pickle.dump(pipe, open(saveModel, 'wb'))\n",
    "\n",
    "        pred_y_test = pipe.predict(X_test)\n",
    "\n",
    "    test_acc, test_f1_macro = getPerformanceMetrics(y_test, pred_y_test, labelList=labelList)\n",
    "\n",
    "    if tunedParms is None:\n",
    "        return grid_t.best_params_\n",
    "            \n",
    "    return test_acc, test_f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model 3] TF-IDF + Naive bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modelThree(X_train, y_train, X_test, y_test, tunedParms=None, saveModel=False, labelList=None):\n",
    "\n",
    "    print('='*5, \"\\tModel 3\\t\", '='*5)\n",
    "\n",
    "    # import and instantiate Multinomial Naive Bayes (with the default parameters)\n",
    "    nb_t = MultinomialNB()\n",
    "    vect_t = TfidfVectorizer() \n",
    "    pipe = make_pipeline(vect_t, nb_t)\n",
    "    pipe.steps\n",
    "\n",
    "    # create a grid of parameters to search\n",
    "    param_grid = {}\n",
    "    param_grid['tfidfvectorizer__token_pattern'] = ['(?u)\\\\b\\\\w\\\\w+\\\\b', '\\\\b[^\\\\d\\\\W]+\\\\b', '([a-z ]+)']\n",
    "    param_grid['tfidfvectorizer__stop_words'] = [\"english\"]\n",
    "    param_grid['tfidfvectorizer__ngram_range'] = [(1, 1),(1,2)]\n",
    "    param_grid['tfidfvectorizer__max_df'] = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    param_grid['tfidfvectorizer__min_df'] = [3, 4, 5]\n",
    "    param_grid['multinomialnb__alpha'] = [0.2, 0.5, 0.7, 1]\n",
    "\n",
    "    if tunedParms is None:\n",
    "\n",
    "        # pass the pipeline (instead of the model) to GridSearchCV\n",
    "        grid_t = GridSearchCV(pipe, param_grid, cv=5, scoring=evaluationMethod, return_train_score=True)\n",
    "        %time grid_t.fit(X_train, y_train)\n",
    "\n",
    "        # examine the score for each combination of parameters\n",
    "        print(grid_t.best_score_)\n",
    "        print(grid_t.best_params_)\n",
    "\n",
    "        pred_y_test = grid_t.predict(X_test)\n",
    "        \n",
    "    else:\n",
    "\n",
    "        pipe.set_params(**tunedParms)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        if saveModel:\n",
    "            pickle.dump(pipe, open(saveModel, 'wb'))\n",
    "        pred_y_test = pipe.predict(X_test)\n",
    "\n",
    "    test_acc, test_f1_macro = getPerformanceMetrics(y_test, pred_y_test, labelList=labelList)\n",
    "\n",
    "    if tunedParms is None:\n",
    "        return grid_t.best_params_\n",
    "\n",
    "    return test_acc, test_f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model 4] BoW + GLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modelFour(X_train, y_train, X_test, y_test, tunedParms=None, saveModel=False, labelList=None):\n",
    "    \n",
    "    print('='*5, \"\\tModel 4\\t\", '='*5)\n",
    "\n",
    "    # GLM+CountVect\n",
    "    glm = LogisticRegression(class_weight='balanced', solver='lbfgs', max_iter=2000, multi_class='multinomial')\n",
    "    vect_t = CountVectorizer() \n",
    "    pipe = make_pipeline(vect_t, glm)\n",
    "    pipe.steps\n",
    "\n",
    "    # create a grid of parameters to search\n",
    "    param_grid = {}\n",
    "    param_grid['countvectorizer__token_pattern'] = ['(?u)\\\\b\\\\w\\\\w+\\\\b', '\\\\b[^\\\\d\\\\W]+\\\\b', '([a-z ]+)']\n",
    "    param_grid['countvectorizer__stop_words'] = [\"english\"]\n",
    "    param_grid['countvectorizer__ngram_range'] = [(1, 1),(1,2)]\n",
    "    param_grid['countvectorizer__max_df'] = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    param_grid['countvectorizer__min_df'] = [3, 4, 5]\n",
    "    param_grid['logisticregression__C'] = [0.1, 0.3, 0.5, 0.7, 0.9] \n",
    "\n",
    "    if tunedParms is None:\n",
    "\n",
    "        # pass the pipeline (instead of the model) to GridSearchCV\n",
    "        grid_t = GridSearchCV(pipe, param_grid, cv=5, scoring=evaluationMethod, return_train_score=True)\n",
    "        %time grid_t.fit(X_train, y_train)\n",
    "\n",
    "        # examine the score for each combination of parameters\n",
    "        print(grid_t.best_score_)\n",
    "        print(grid_t.best_params_)\n",
    "        pred_y_test = grid_t.predict(X_test)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        pipe.set_params(**tunedParms)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        if saveModel:\n",
    "            pickle.dump(pipe, open(saveModel, 'wb'))\n",
    "        pred_y_test = pipe.predict(X_test)\n",
    "\n",
    "    test_acc, test_f1_macro = getPerformanceMetrics(y_test, pred_y_test, labelList=labelList)\n",
    "\n",
    "    if tunedParms is None:\n",
    "        return grid_t.best_params_\n",
    "    \n",
    "    return test_acc, test_f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model 5] TF-IDF + GLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modelFive(X_train, y_train, X_test, y_test, tunedParms=None, saveModel=False, labelList=None):\n",
    "    \n",
    "    print('='*5, \"\\tModel 5\\t\", '='*5)\n",
    "\n",
    "    # GLM+TFIDFVect\n",
    "    glm = LogisticRegression(class_weight='balanced', solver='lbfgs', max_iter=2000, multi_class='multinomial')\n",
    "    vect_t = TfidfVectorizer() \n",
    "    pipe = make_pipeline(vect_t, glm)\n",
    "    pipe.steps\n",
    "\n",
    "    # create a grid of parameters to search (and specify the pipeline step along with the parameter)\n",
    "    param_grid = {}\n",
    "    param_grid['tfidfvectorizer__token_pattern'] = ['(?u)\\\\b\\\\w\\\\w+\\\\b', '\\\\b[^\\\\d\\\\W]+\\\\b', '([a-z ]+)']\n",
    "    param_grid['tfidfvectorizer__stop_words'] = [\"english\"]\n",
    "    param_grid['tfidfvectorizer__ngram_range'] = [(1, 1),(1,2)]\n",
    "    param_grid['tfidfvectorizer__max_df'] = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    param_grid['tfidfvectorizer__min_df'] = [3, 4, 5]\n",
    "    param_grid['logisticregression__C'] = [0.1, 0.3, 0.5, 0.7, 0.9] \n",
    "\n",
    "    if tunedParms is None:\n",
    "\n",
    "        # pass the pipeline (instead of the model) to GridSearchCV\n",
    "        grid_t = GridSearchCV(pipe, param_grid, cv=5, scoring=evaluationMethod, return_train_score=True)\n",
    "        %time grid_t.fit(X_train, y_train)\n",
    "\n",
    "        # examine the score for each combination of parameters\n",
    "        print(grid_t.best_score_)\n",
    "        print(grid_t.best_params_)\n",
    "\n",
    "        pred_y_test = grid_t.predict(X_test)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        pipe.set_params(**tunedParms)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        if saveModel:\n",
    "            pickle.dump(pipe, open(saveModel, 'wb'))\n",
    "        pred_y_test = pipe.predict(X_test)\n",
    "\n",
    "    test_acc, test_f1_macro = getPerformanceMetrics(y_test, pred_y_test, labelList=labelList)\n",
    "\n",
    "    if tunedParms is None:\n",
    "        return grid_t.best_params_\n",
    "    \n",
    "    return test_acc, test_f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model 6] BoW + XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modelSix(X_train, y_train, X_test, y_test, tunedParms=None, saveModel=False, labelList=None):\n",
    "\n",
    "    print('='*5, \"\\tModel 6\\t\", '='*5)\n",
    "\n",
    "    # XGB+CountVect\n",
    "    nclass = np.unique(y_train).shape[0]\n",
    "    xgb = XGBClassifier(objective='multi:softmax', num_class=nclass, random_state=42, n_jobs=4)\n",
    "    vect_t = CountVectorizer()\n",
    "    pipe = make_pipeline(vect_t, xgb)\n",
    "\n",
    "    # create a grid of parameters to search (and specify the pipeline step along with the parameter)\n",
    "    param_grid = {}\n",
    "    param_grid['countvectorizer__token_pattern'] = ['(?u)\\\\b\\\\w\\\\w+\\\\b']\n",
    "    param_grid['countvectorizer__stop_words'] = [\"english\"]\n",
    "    param_grid['countvectorizer__ngram_range'] = [(1, 1),(1,2)]\n",
    "    param_grid['countvectorizer__max_df'] = [0.5]\n",
    "    param_grid['countvectorizer__min_df'] = [3, 4, 5]\n",
    "    param_grid['xgbclassifier__learning_rate'] = [0.1, 0.05] \n",
    "    param_grid['xgbclassifier__max_depth'] = [3,5] \n",
    "    param_grid['xgbclassifier__n_estimators'] = [50,100,200] \n",
    "\n",
    "    if tunedParms is None:\n",
    "        # pass the pipeline (instead of the model) to GridSearchCV\n",
    "        grid_t = GridSearchCV(pipe, param_grid, cv=5, scoring=evaluationMethod, return_train_score=True)\n",
    "        %time grid_t.fit(X_train, y_train)\n",
    "        \n",
    "        # examine the score for each combination of parameters\n",
    "        print(grid_t.best_score_)\n",
    "        print(grid_t.best_params_)\n",
    "        pred_y_test = grid_t.predict(X_test)\n",
    "        \n",
    "    else:\n",
    "        pipe.set_params(**tunedParms)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        if saveModel:\n",
    "            pickle.dump(pipe, open(saveModel, 'wb'))\n",
    "        pred_y_test = pipe.predict(X_test)\n",
    "        \n",
    "    test_acc, test_f1_macro = getPerformanceMetrics(y_test, pred_y_test, labelList=labelList)\n",
    "\n",
    "    if tunedParms is None:\n",
    "        return grid_t.best_params_\n",
    "\n",
    "    return test_acc, test_f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model 7] BoW + 1-hidden-layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def modelSeven(X_train, y_train, X_test, y_test, tunedParms=None, saveModel=False, labelList=None):\n",
    "    \n",
    "    print('='*5, \"\\tModel 7\\t\", '='*5)\n",
    "\n",
    "    enc = LabelEncoder()\n",
    "    enc.fit(y_train)\n",
    "    vect = CountVectorizer()\n",
    "    nn = kerasDenseNNWrapper(dummyEnc=enc)\n",
    "    pipe = make_pipeline(vect, nn)\n",
    "\n",
    "    # create a grid of parameters to search (and specify the pipeline step along with the parameter)\n",
    "    param_grid = {}\n",
    "    param_grid['countvectorizer__token_pattern'] = ['(?u)\\\\b\\\\w\\\\w+\\\\b']\n",
    "    param_grid['countvectorizer__stop_words'] = [\"english\"]\n",
    "    param_grid['countvectorizer__ngram_range'] = [(1, 1),(1,2)]\n",
    "    param_grid['countvectorizer__max_df'] = [0.5]\n",
    "    param_grid['countvectorizer__min_df'] = [3, 4, 5]\n",
    "    param_grid['kerasdensennwrapper__n_neuron'] = [[10], [20], [40]] # One layer\n",
    "    param_grid['kerasdensennwrapper__rate'] = [0.3, 0.5, 0.8] \n",
    "\n",
    "    if tunedParms is None:\n",
    "        # pass the pipeline (instead of the model) to GridSearchCV\n",
    "        grid_t = GridSearchCV(pipe, param_grid, cv=5, scoring=evaluationMethod, return_train_score=True)\n",
    "        %time grid_t.fit(X_train, y_train)\n",
    "        \n",
    "        # examine the score for each combination of parameters\n",
    "        print(grid_t.best_score_)\n",
    "        print(grid_t.best_params_)\n",
    "        pred_y_test = grid_t.predict(X_test)\n",
    "        \n",
    "    else:\n",
    "        pipe.set_params(**tunedParms)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        if saveModel:\n",
    "            pickle.dump(pipe, open(saveModel, 'wb'))\n",
    "        pred_y_test = pipe.predict(X_test)\n",
    "        \n",
    "    test_acc, test_f1_macro = getPerformanceMetrics(y_test, pred_y_test, labelList=labelList)\n",
    "\n",
    "    if tunedParms is None:\n",
    "        return grid_t.best_params_\n",
    "\n",
    "    return test_acc, test_f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model 8] CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def modelEight(X_train, y_train, X_test, y_test, tunedParms=None, saveModel=False, labelList=None):\n",
    "    \n",
    "    print('='*5, \"\\tModel 8\\t\", '='*5)\n",
    "    \n",
    "    enc = LabelEncoder()\n",
    "    enc.fit(y_train)\n",
    "    nn = kerasCNNWrapper(dummyEnc=enc)\n",
    "    pipe = make_pipeline(nn)\n",
    "\n",
    "    # create a grid of parameters to search (and specify the pipeline step along with the parameter)\n",
    "    param_grid = {}\n",
    "    param_grid['kerascnnwrapper__vocabsize'] = [1000, 1200, 1500] # One layer\n",
    "    param_grid['kerascnnwrapper__embeddingdim'] = [15, 20] \n",
    "    param_grid['kerascnnwrapper__filtersizes'] = [[2, 3], [2,3,4]]\n",
    "    param_grid['kerascnnwrapper__numfilters'] = [2,3]\n",
    "\n",
    "    if tunedParms is None:\n",
    "        # pass the pipeline (instead of the model) to GridSearchCV\n",
    "        grid_t = GridSearchCV(pipe, param_grid, cv=5, scoring=evaluationMethod, return_train_score=True)\n",
    "        %time grid_t.fit(X_train, y_train)\n",
    "        \n",
    "        # examine the score for each combination of parameters\n",
    "        print(grid_t.best_score_)\n",
    "        print(grid_t.best_params_)\n",
    "        pred_y_test = grid_t.predict(X_test)\n",
    "        \n",
    "    else:\n",
    "        pipe.set_params(**tunedParms)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        if saveModel:\n",
    "            pickle.dump(pipe, open(saveModel, 'wb'))\n",
    "        pred_y_test = pipe.predict(X_test)\n",
    "        \n",
    "    test_acc, test_f1_macro = getPerformanceMetrics(y_test, pred_y_test, labelList=labelList)\n",
    "\n",
    "    if tunedParms is None:\n",
    "        return grid_t.best_params_\n",
    "\n",
    "    return test_acc, test_f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model 9] Manual features + GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modelNine(X_train, y_train, X_test, y_test, tunedParms=None, saveModel=False, labelList=None):\n",
    "    # input is X_train_withEmoji\n",
    "\n",
    "    print('='*5, \"\\tModel 9\\t\", '='*5)\n",
    "\n",
    "    # GLM + manual features\n",
    "    glm = LogisticRegression(class_weight='balanced', solver='lbfgs', max_iter=2000, multi_class='multinomial')\n",
    "    manualFun = FunctionTransformer(combineFeatures, validate=False)\n",
    "    manualEmoji = FunctionTransformer(makeFeatureEmoji, validate=False)\n",
    "    vect_t = CountVectorizer()\n",
    "    featuresUnion = make_union(manualFun, make_pipeline(manualEmoji, vect_t))\n",
    "    pipe = make_pipeline(featuresUnion, glm)\n",
    "\n",
    "    # create a grid of parameters to search\n",
    "    param_grid = {}\n",
    "    param_grid['logisticregression__C'] = [0.1, 0.3, 0.5, 0.7, 0.9] \n",
    "    param_grid['featureunion__pipeline__countvectorizer__token_pattern'] = ['(?u)\\\\b\\\\w\\\\w+\\\\b']\n",
    "    param_grid['featureunion__pipeline__countvectorizer__stop_words'] = [\"english\"]\n",
    "    param_grid['featureunion__pipeline__countvectorizer__ngram_range'] = [(1, 1), (1,2)]\n",
    "      \n",
    "    #param_grid['featureunion__pipeline__countvectorizer__token_pattern'] = ['(?u)\\\\b\\\\w\\\\w+\\\\b', '\\\\b[^\\\\d\\\\W]+\\\\b', '([a-z ]+)']\n",
    "    #param_grid['featureunion__pipeline__countvectorizer__stop_words'] = [\"english\"]\n",
    "    #param_grid['featureunion__pipeline__countvectorizer__ngram_range'] = [(1, 1),(1,2)]\n",
    "    #param_grid['featureunion__pipeline__countvectorizer__max_df'] = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    #param_grid['featureunion__pipeline__countvectorizer__min_df'] = [3, 4, 5]\n",
    "\n",
    "    if tunedParms is None:\n",
    "\n",
    "        # Start cross validation to tune hyperparameters\n",
    "        grid_t = GridSearchCV(pipe, param_grid, cv=5, scoring=evaluationMethod, return_train_score=True)\n",
    "        %time grid_t.fit(X_train, y_train)\n",
    "\n",
    "        # examine the best score\n",
    "        print(grid_t.best_score_)\n",
    "        print(grid_t.best_params_)\n",
    "\n",
    "        pred_y_test = grid_t.predict(X_test)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        pipe.set_params(**tunedParms)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        if saveModel:\n",
    "            pickle.dump(pipe, open(saveModel, 'wb'))\n",
    "        pred_y_test = pipe.predict(X_test)\n",
    "\n",
    "    test_acc, test_f1_macro = getPerformanceMetrics(y_test, pred_y_test, labelList=labelList)\n",
    "\n",
    "    if tunedParms is None:\n",
    "        return grid_t.best_params_\n",
    "            \n",
    "    return test_acc, test_f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model 10] Manual features + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def modelTen(X_train, y_train, X_test, y_test, tunedParms=None, saveModel=False, labelList=None):\n",
    "    # input is X_train_withEmoji\n",
    "    \n",
    "    print('='*5, \"\\tModel 10\\t\", '='*5)\n",
    "    # XGB + manual features\n",
    "    nclass = np.unique(y_train).shape[0]\n",
    "    xgb = XGBClassifier(objective='multi:softmax', num_class=nclass, random_state=42, n_jobs=4)\n",
    "    manualFun = FunctionTransformer(combineFeatures, validate=False)\n",
    "    manualEmoji = FunctionTransformer(makeFeatureEmoji, validate=False)\n",
    "    vect_t = CountVectorizer()\n",
    "    featuresUnion = make_union(manualFun, make_pipeline(manualEmoji, vect_t))\n",
    "    pipe = make_pipeline(featuresUnion, xgb)\n",
    "\n",
    "    # create a grid of parameters to search\n",
    "    param_grid = {}\n",
    "    param_grid['xgbclassifier__learning_rate'] = [0.1, 0.05] \n",
    "    param_grid['xgbclassifier__max_depth'] = [3,5] \n",
    "    param_grid['xgbclassifier__n_estimators'] = [50,100,200] \n",
    "    param_grid['featureunion__pipeline__countvectorizer__token_pattern'] = ['(?u)\\\\b\\\\w\\\\w+\\\\b']\n",
    "    param_grid['featureunion__pipeline__countvectorizer__stop_words'] = [\"english\"]\n",
    "    param_grid['featureunion__pipeline__countvectorizer__ngram_range'] = [(1, 1)]\n",
    "    \n",
    "    if tunedParms is None:\n",
    "\n",
    "        # Start cross validation to tune hyperparameters\n",
    "        grid_t = GridSearchCV(pipe, param_grid, cv=5, scoring=evaluationMethod, return_train_score=True)\n",
    "        %time grid_t.fit(X_train, y_train)\n",
    "\n",
    "        # examine the best score\n",
    "        print(grid_t.best_score_)\n",
    "        print(grid_t.best_params_)\n",
    "\n",
    "        pred_y_test = grid_t.predict(X_test)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        pipe.set_params(**tunedParms)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        if saveModel:\n",
    "            pickle.dump(pipe, open(saveModel, 'wb'))\n",
    "        pred_y_test = pipe.predict(X_test)\n",
    "\n",
    "    test_acc, test_f1_macro = getPerformanceMetrics(y_test, pred_y_test, labelList=labelList)\n",
    "\n",
    "    if tunedParms is None:\n",
    "        return grid_t.best_params_\n",
    "            \n",
    "    return test_acc, test_f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Problem 1</u>  --  Predicting the type of feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate X and y for topic\n",
    "y_train = labelled_tweets_Y_train.feedback\n",
    "y_test = labelled_tweets_Y_test.feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \tModel 1\t =====\n",
      "Test accuracy\t0.543\n",
      "F1 micro\t0.543\n",
      "F1 macro\t0.513\n",
      "Confusion matrix\n",
      " [[100  10  29  27]\n",
      " [  4  27   6   9]\n",
      " [ 13   2  53  16]\n",
      " [ 57  42  59 146]]\n",
      "===== \tModel 2\t =====\n",
      "Test accuracy\t0.608\n",
      "F1 micro\t0.608\n",
      "F1 macro\t0.520\n",
      "Confusion matrix\n",
      " [[102   5  23  36]\n",
      " [  3  13   2  28]\n",
      " [ 24   0  39  21]\n",
      " [ 62  14  17 211]]\n",
      "===== \tModel 3\t =====\n",
      "Test accuracy\t0.628\n",
      "F1 micro\t0.628\n",
      "F1 macro\t0.454\n",
      "Confusion matrix\n",
      " [[ 90   1  13  62]\n",
      " [  2   3   2  39]\n",
      " [ 21   0  26  37]\n",
      " [ 36   1   9 258]]\n",
      "===== \tModel 4\t =====\n",
      "Test accuracy\t0.633\n",
      "F1 micro\t0.633\n",
      "F1 macro\t0.563\n",
      "Confusion matrix\n",
      " [[ 89  14  29  34]\n",
      " [  5  29   2  10]\n",
      " [ 17   5  40  22]\n",
      " [ 33  31  18 222]]\n",
      "===== \tModel 5\t =====\n",
      "Test accuracy\t0.610\n",
      "F1 micro\t0.610\n",
      "F1 macro\t0.550\n",
      "Confusion matrix\n",
      " [[ 97  13  30  26]\n",
      " [  3  28   5  10]\n",
      " [ 18   5  44  17]\n",
      " [ 43  38  26 197]]\n",
      "===== \tModel 6\t =====\n",
      "Test accuracy\t0.658\n",
      "F1 micro\t0.658\n",
      "F1 macro\t0.527\n",
      "Confusion matrix\n",
      " [[ 90   3  14  59]\n",
      " [  6  15   2  23]\n",
      " [ 22   3  21  38]\n",
      " [ 24   5   6 269]]\n",
      "===== \tModel 7\t =====\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Test accuracy\t0.645\n",
      "F1 micro\t0.645\n",
      "F1 macro\t0.502\n",
      "Confusion matrix\n",
      " [[ 85   2  12  67]\n",
      " [  4  10   2  30]\n",
      " [ 24   0  23  37]\n",
      " [ 25   3   7 269]]\n",
      "===== \tModel 8\t =====\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Test accuracy\t0.552\n",
      "F1 micro\t0.552\n",
      "F1 macro\t0.326\n",
      "Confusion matrix\n",
      " [[ 35   0   6 125]\n",
      " [  1   0   1  44]\n",
      " [ 16   0  19  49]\n",
      " [ 14   0  13 277]]\n",
      "===== \tModel 9\t =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy\t0.450\n",
      "F1 micro\t0.450\n",
      "F1 macro\t0.426\n",
      "Confusion matrix\n",
      " [[ 93  22  30  21]\n",
      " [  7  27   9   3]\n",
      " [ 14  10  44  16]\n",
      " [ 43  83  72 106]]\n",
      "===== \tModel 10\t =====\n",
      "Test accuracy\t0.597\n",
      "F1 micro\t0.597\n",
      "F1 macro\t0.360\n",
      "Confusion matrix\n",
      " [[ 78   1   2  85]\n",
      " [  6   2   1  37]\n",
      " [ 12   1   5  66]\n",
      " [ 24   5   2 273]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TestAcc</th>\n",
       "      <th>TestF1Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.543333</td>\n",
       "      <td>0.513495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.520206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.628333</td>\n",
       "      <td>0.453570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.562655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.550210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.527031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.501854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.551667</td>\n",
       "      <td>0.326008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.425599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.596667</td>\n",
       "      <td>0.359573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TestAcc  TestF1Macro\n",
       "0  0.543333  0.513495   \n",
       "1  0.608333  0.520206   \n",
       "2  0.628333  0.453570   \n",
       "3  0.633333  0.562655   \n",
       "4  0.610000  0.550210   \n",
       "5  0.658333  0.527031   \n",
       "6  0.645000  0.501854   \n",
       "7  0.551667  0.326008   \n",
       "8  0.450000  0.425599   \n",
       "9  0.596667  0.359573   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Hyperparameter tuning using 5-fold cross validation ###\n",
    "tuning = False\n",
    "if tuning:\n",
    "    modelOneBestParms = modelOne(X_train, y_train, X_test, y_test)\n",
    "    print('modelOneBestParms = {}\\n\\n'.format(modelOneBestParms))\n",
    "\n",
    "    modelTwoBestParms = modelTwo(X_train, y_train, X_test, y_test)\n",
    "    print('modelTwoBestParms = {}\\n\\n'.format(modelTwoBestParms))\n",
    "\n",
    "    modelThreeBestParms = modelThree(X_train, y_train, X_test, y_test)\n",
    "    print('modelThreeBestParms = {}\\n\\n'.format(modelThreeBestParms))\n",
    "\n",
    "    modelFourBestParms = modelFour(X_train, y_train, X_test, y_test)\n",
    "    print('modelFourBestParms = {}\\n\\n'.format(modelFourBestParms))\n",
    "\n",
    "    modelFiveBestParms = modelFive(X_train, y_train, X_test, y_test)\n",
    "    print('modelFiveBestParms = {}\\n\\n'.format(modelFiveBestParms))\n",
    "\n",
    "    modelSixBestParms = modelSix(X_train, y_train, X_test, y_test)\n",
    "    print('modelSixBestParms = {}\\n\\n'.format(modelSixBestParms))\n",
    "\n",
    "    modelSevenBestParms = modelSeven(X_train, y_train, X_test, y_test)\n",
    "    print('modelSevenBestParms = {}\\n\\n'.format(modelSevenBestParms))\n",
    "\n",
    "    modelEightBestParms = modelEight(X_train, y_train, X_test, y_test)\n",
    "    print('modelEightBestParms = {}\\n\\n'.format(modelEightBestParms))\n",
    "    \n",
    "    modelNineBestParms = modelNine(X_train_withEmoji, y_train, X_test_withEmoji, y_test)\n",
    "    print('modelNineBestParms = {}\\n\\n'.format(modelNineBestParms))\n",
    "    \n",
    "    modelTenBestParms = modelTen(X_train_withEmoji, y_train, X_test_withEmoji, y_test)\n",
    "    print('modelTenBestParms = {}\\n\\n'.format(modelTenBestParms))\n",
    "\n",
    "    \n",
    "### Use tuned hyperparameter ###\n",
    "modelOneBestParms = {'logisticregression__C': 0.1}\n",
    "modelTwoBestParms = {'countvectorizer__max_df': 0.5, 'countvectorizer__min_df': 3, \n",
    "                     'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', \n",
    "                     'countvectorizer__token_pattern': '\\\\b[^\\\\d\\\\W]+\\\\b', 'multinomialnb__alpha': 0.5}\n",
    "modelThreeBestParms = {'multinomialnb__alpha': 0.2, 'tfidfvectorizer__max_df': 0.5, 'tfidfvectorizer__min_df': 5, \n",
    "                       'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__stop_words': 'english', \n",
    "                       'tfidfvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b'}\n",
    "modelFourBestParms = {'countvectorizer__max_df': 0.5, 'countvectorizer__min_df': 3, 'countvectorizer__ngram_range': (1, 2), \n",
    "                      'countvectorizer__stop_words': 'english', 'countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', \n",
    "                      'logisticregression__C': 0.1}\n",
    "modelFiveBestParms = {'logisticregression__C': 0.9, 'tfidfvectorizer__max_df': 0.5, 'tfidfvectorizer__min_df': 3, \n",
    "                      'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__stop_words': 'english', \n",
    "                      'tfidfvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b'}\n",
    "modelSixBestParms = {'countvectorizer__max_df': 0.5, 'countvectorizer__min_df': 5, 'countvectorizer__ngram_range': (1, 2), \n",
    "                     'countvectorizer__stop_words': 'english',  'countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', \n",
    "                     'xgbclassifier__learning_rate': 0.1, 'xgbclassifier__max_depth': 5, 'xgbclassifier__n_estimators': 200}\n",
    "modelSevenBestParms = {'countvectorizer__max_df': 0.5, 'countvectorizer__min_df': 4, 'countvectorizer__ngram_range': (1, 2), \n",
    "                       'countvectorizer__stop_words': 'english', 'countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', \n",
    "                       'kerasdensennwrapper__n_neuron': [40], 'kerasdensennwrapper__rate': 0.3}\n",
    "modelEightBestParms = {'kerascnnwrapper__embeddingdim': 20, 'kerascnnwrapper__filtersizes': [2, 3, 4], \n",
    "                       'kerascnnwrapper__numfilters': 3, 'kerascnnwrapper__vocabsize': 1200}\n",
    "modelNineBestParms = {'featureunion__pipeline__countvectorizer__ngram_range': (1, 2), 'featureunion__pipeline__countvectorizer__stop_words': 'english', \n",
    "                      'featureunion__pipeline__countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'logisticregression__C': 0.1}\n",
    "modelTenBestParms = {'featureunion__pipeline__countvectorizer__ngram_range': (1, 1), 'featureunion__pipeline__countvectorizer__stop_words': 'english', \n",
    "                     'featureunion__pipeline__countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', \n",
    "                     'xgbclassifier__learning_rate': 0.05, 'xgbclassifier__max_depth': 3, 'xgbclassifier__n_estimators': 50}\n",
    "\n",
    "\n",
    "\n",
    "### Evaluate model performance with tuned parameters ###\n",
    "opts = {'labelList': feedbackLabels}\n",
    "finalResults = pd.DataFrame(columns=['TestAcc', 'TestF1Macro']) # To store all results\n",
    "\n",
    "acc, f1macro = modelOne(X_train, y_train, X_test, y_test, tunedParms=modelOneBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelTwo(X_train, y_train, X_test, y_test, tunedParms=modelTwoBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelThree(X_train, y_train, X_test, y_test, tunedParms=modelThreeBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelFour(X_train, y_train, X_test, y_test, tunedParms=modelFourBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelFive(X_train, y_train, X_test, y_test, tunedParms=modelFiveBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelSix(X_train, y_train, X_test, y_test, tunedParms=modelSixBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelSeven(X_train, y_train, X_test, y_test, tunedParms=modelSevenBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelEight(X_train, y_train, X_test, y_test, tunedParms=modelEightBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelNine(X_train_withEmoji, y_train, X_test_withEmoji, y_test, tunedParms=modelNineBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelTen(X_train_withEmoji, y_train, X_test_withEmoji, y_test, tunedParms=modelTenBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "display(finalResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the best model for live prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \tModel 4\t =====\n",
      "Test accuracy\t0.633\n",
      "F1 micro\t0.633\n",
      "F1 macro\t0.563\n",
      "Confusion matrix\n",
      " [[ 89  14  29  34]\n",
      " [  5  29   2  10]\n",
      " [ 17   5  40  22]\n",
      " [ 33  31  18 222]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6333333333333333, 0.5626553235129592)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFour(X_train, y_train, X_test, y_test, tunedParms=modelFourBestParms, saveModel=os.path.join(outFolder, 'BestModel_Feedback.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Problem 2</u>  --  Predicting topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate X and y for topic\n",
    "y_train = labelled_tweets_Y_train.topic\n",
    "y_test = labelled_tweets_Y_test.topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \tModel 1\t =====\n",
      "Test accuracy\t0.473\n",
      "F1 micro\t0.473\n",
      "F1 macro\t0.431\n",
      "Confusion matrix\n",
      " [[138  29  48  27  21  48   9  13]\n",
      " [  1   7   0   2   1   0   0   2]\n",
      " [ 12   1  29   4   6   2   4   1]\n",
      " [  7   4   5  28  10   4   5   0]\n",
      " [  4   0   2   5  40   6   1   0]\n",
      " [  7   2   3   3   5  11   3   1]\n",
      " [  0   0   3   0   2   0  23   0]\n",
      " [  0   0   1   1   0   1   0   8]]\n",
      "===== \tModel 2\t =====\n",
      "Test accuracy\t0.668\n",
      "F1 micro\t0.668\n",
      "F1 macro\t0.476\n",
      "Confusion matrix\n",
      " [[285   3   7  15  13   6   3   1]\n",
      " [  2   5   1   4   1   0   0   0]\n",
      " [ 24   0  21   4   4   3   3   0]\n",
      " [ 18   0   1  37   5   1   0   1]\n",
      " [ 16   0   1   6  34   0   1   0]\n",
      " [ 25   1   2   2   4   1   0   0]\n",
      " [  8   0   1   0   4   0  15   0]\n",
      " [  1   0   1   4   1   0   1   3]]\n",
      "===== \tModel 3\t =====\n",
      "Test accuracy\t0.640\n",
      "F1 micro\t0.640\n",
      "F1 macro\t0.324\n",
      "Confusion matrix\n",
      " [[317   1   2  10   1   2   0   0]\n",
      " [ 10   2   0   1   0   0   0   0]\n",
      " [ 44   0  11   2   2   0   0   0]\n",
      " [ 34   0   0  26   2   1   0   0]\n",
      " [ 31   0   0   3  23   0   1   0]\n",
      " [ 32   0   0   2   1   0   0   0]\n",
      " [ 19   0   1   0   3   0   5   0]\n",
      " [  7   0   1   3   0   0   0   0]]\n",
      "===== \tModel 4\t =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy\t0.638\n",
      "F1 micro\t0.638\n",
      "F1 macro\t0.503\n",
      "Confusion matrix\n",
      " [[250   3  16  27  10  18   4   5]\n",
      " [  3   6   1   2   1   0   0   0]\n",
      " [ 19   0  24   4   5   3   4   0]\n",
      " [ 12   0   1  38   4   2   4   2]\n",
      " [  8   0   1  11  33   4   1   0]\n",
      " [ 15   1   3   4   2   8   1   1]\n",
      " [  2   0   3   1   2   0  20   0]\n",
      " [  2   0   1   3   1   0   0   4]]\n",
      "===== \tModel 5\t =====\n",
      "Test accuracy\t0.582\n",
      "F1 micro\t0.582\n",
      "F1 macro\t0.520\n",
      "Confusion matrix\n",
      " [[202   6  21  29  15  44   9   7]\n",
      " [  1  10   0   2   0   0   0   0]\n",
      " [ 15   1  24   3   8   4   4   0]\n",
      " [ 10   0   4  34   5   4   4   2]\n",
      " [  3   0   1  10  38   5   1   0]\n",
      " [ 11   1   1   3   2  13   2   2]\n",
      " [  0   0   3   1   3   0  21   0]\n",
      " [  2   0   1   1   0   0   0   7]]\n",
      "===== \tModel 6\t =====\n",
      "Test accuracy\t0.683\n",
      "F1 micro\t0.683\n",
      "F1 macro\t0.494\n",
      "Confusion matrix\n",
      " [[315   2   3   7   1   3   1   1]\n",
      " [  7   5   0   1   0   0   0   0]\n",
      " [ 34   0  16   3   3   0   3   0]\n",
      " [ 33   0   3  19   4   1   2   1]\n",
      " [ 23   0   0   5  28   1   1   0]\n",
      " [ 22   1   1   0   3   6   1   1]\n",
      " [  7   0   1   0   2   1  17   0]\n",
      " [  3   0   1   2   1   0   0   4]]\n",
      "===== \tModel 7\t =====\n",
      "Test accuracy\t0.657\n",
      "F1 micro\t0.657\n",
      "F1 macro\t0.384\n",
      "Confusion matrix\n",
      " [[313   1   5  12   1   1   0   0]\n",
      " [  8   4   0   1   0   0   0   0]\n",
      " [ 36   0  16   2   4   0   1   0]\n",
      " [ 34   0   3  18   4   2   1   1]\n",
      " [ 23   0   1   3  30   0   1   0]\n",
      " [ 27   1   3   1   3   0   0   0]\n",
      " [  7   0   4   0   4   0  13   0]\n",
      " [  6   0   1   2   2   0   0   0]]\n",
      "===== \tModel 8\t =====\n",
      "Test accuracy\t0.583\n",
      "F1 micro\t0.583\n",
      "F1 macro\t0.184\n",
      "Confusion matrix\n",
      " [[320   0   4   6   1   0   2   0]\n",
      " [ 13   0   0   0   0   0   0   0]\n",
      " [ 39   0  15   4   1   0   0   0]\n",
      " [ 49   0   2  12   0   0   0   0]\n",
      " [ 46   0   9   2   1   0   0   0]\n",
      " [ 32   0   1   2   0   0   0   0]\n",
      " [ 16   0   2   8   0   0   2   0]\n",
      " [  7   0   1   3   0   0   0   0]]\n",
      "===== \tModel 9\t =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy\t0.382\n",
      "F1 micro\t0.382\n",
      "F1 macro\t0.181\n",
      "Confusion matrix\n",
      " [[194  36   8  15   7  27  14  32]\n",
      " [  2   0   2   3   0   1   1   4]\n",
      " [ 10   6   7   2  11   3   7  13]\n",
      " [ 13  11   0   5   8   5   8  13]\n",
      " [ 11   6   4   1   8   4  14  10]\n",
      " [  9   2   2   2   3   8   4   5]\n",
      " [  2   5   2   5   2   5   5   2]\n",
      " [  2   0   2   1   1   2   1   2]]\n",
      "===== \tModel 10\t =====\n",
      "Test accuracy\t0.558\n",
      "F1 micro\t0.558\n",
      "F1 macro\t0.121\n",
      "Confusion matrix\n",
      " [[326   1   2   1   3   0   0   0]\n",
      " [ 12   0   0   1   0   0   0   0]\n",
      " [ 52   0   1   1   5   0   0   0]\n",
      " [ 52   0   3   2   4   0   2   0]\n",
      " [ 47   0   3   2   6   0   0   0]\n",
      " [ 35   0   0   0   0   0   0   0]\n",
      " [ 25   0   1   0   2   0   0   0]\n",
      " [  9   0   1   1   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TestAcc</th>\n",
       "      <th>TestF1Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.473333</td>\n",
       "      <td>0.430894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.668333</td>\n",
       "      <td>0.475646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.323985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.638333</td>\n",
       "      <td>0.503220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.581667</td>\n",
       "      <td>0.520045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.493653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.384274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.183613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.381667</td>\n",
       "      <td>0.180746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.121315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TestAcc  TestF1Macro\n",
       "0  0.473333  0.430894   \n",
       "1  0.668333  0.475646   \n",
       "2  0.640000  0.323985   \n",
       "3  0.638333  0.503220   \n",
       "4  0.581667  0.520045   \n",
       "5  0.683333  0.493653   \n",
       "6  0.656667  0.384274   \n",
       "7  0.583333  0.183613   \n",
       "8  0.381667  0.180746   \n",
       "9  0.558333  0.121315   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Hyperparameter tuning using 5-fold cross validation ###\n",
    "tuning = False\n",
    "if tuning:\n",
    "    modelOneBestParms = modelOne(X_train, y_train, X_test, y_test)\n",
    "    print('modelOneBestParms = {}\\n\\n'.format(modelOneBestParms))\n",
    "\n",
    "    modelTwoBestParms = modelTwo(X_train, y_train, X_test, y_test)\n",
    "    print('modelTwoBestParms = {}\\n\\n'.format(modelTwoBestParms))\n",
    "\n",
    "    modelThreeBestParms = modelThree(X_train, y_train, X_test, y_test)\n",
    "    print('modelThreeBestParms = {}\\n\\n'.format(modelThreeBestParms))\n",
    "\n",
    "    modelFourBestParms = modelFour(X_train, y_train, X_test, y_test)\n",
    "    print('modelFourBestParms = {}\\n\\n'.format(modelFourBestParms))\n",
    "\n",
    "    modelFiveBestParms = modelFive(X_train, y_train, X_test, y_test)\n",
    "    print('modelFiveBestParms = {}\\n\\n'.format(modelFiveBestParms))\n",
    "\n",
    "    modelSixBestParms = modelSix(X_train, y_train, X_test, y_test)\n",
    "    print('modelSixBestParms = {}\\n\\n'.format(modelSixBestParms))\n",
    "\n",
    "    modelNineBestParms = modelNine(X_train_withEmoji, y_train, X_test_withEmoji, y_test)\n",
    "    print('modelNineBestParms = {}\\n\\n'.format(modelNineBestParms))\n",
    "\n",
    "    modelTenBestParms = modelTen(X_train_withEmoji, y_train, X_test_withEmoji, y_test)\n",
    "    print('modelTenBestParms = {}\\n\\n'.format(modelTenBestParms))\n",
    "\n",
    "    modelSevenBestParms = modelSeven(X_train, y_train, X_test, y_test)\n",
    "    print('modelSevenBestParms = {}\\n\\n'.format(modelSevenBestParms))\n",
    "\n",
    "    modelEightBestParms = modelEight(X_train, y_train, X_test, y_test)\n",
    "    print('modelEightBestParms = {}\\n\\n'.format(modelEightBestParms))\n",
    "\n",
    "\n",
    "### Use tuned hyperparameter ###\n",
    "modelOneBestParms = {'logisticregression__C': 0.1}\n",
    "modelTwoBestParms = {'countvectorizer__max_df': 0.5, 'countvectorizer__min_df': 5, \n",
    "                     'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': 'english', \n",
    "                     'countvectorizer__token_pattern': '\\\\b[^\\\\d\\\\W]+\\\\b', 'multinomialnb__alpha': 1}\n",
    "modelThreeBestParms = {'multinomialnb__alpha': 0.2, 'tfidfvectorizer__max_df': 0.5, 'tfidfvectorizer__min_df': 5, \n",
    "                       'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__stop_words': 'english', \n",
    "                       'tfidfvectorizer__token_pattern': '\\\\b[^\\\\d\\\\W]+\\\\b'}\n",
    "modelFourBestParms = {'countvectorizer__max_df': 0.5, 'countvectorizer__min_df': 3, 'countvectorizer__ngram_range': (1, 2), \n",
    "                      'countvectorizer__stop_words': 'english', 'countvectorizer__token_pattern': '\\\\b[^\\\\d\\\\W]+\\\\b', 'logisticregression__C': 0.7}\n",
    "modelFiveBestParms = {'logisticregression__C': 0.9, 'tfidfvectorizer__max_df': 0.5, 'tfidfvectorizer__min_df': 3, 'tfidfvectorizer__ngram_range': (1, 2), \n",
    "                      'tfidfvectorizer__stop_words': 'english', 'tfidfvectorizer__token_pattern': '\\\\b[^\\\\d\\\\W]+\\\\b'}\n",
    "modelSixBestParms = {'countvectorizer__max_df': 0.5, 'countvectorizer__min_df': 3, 'countvectorizer__ngram_range': (1, 2), \n",
    "                     'countvectorizer__stop_words': 'english', 'countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', \n",
    "                     'xgbclassifier__learning_rate': 0.1, 'xgbclassifier__max_depth': 3, 'xgbclassifier__n_estimators': 200}\n",
    "modelSevenBestParms = {'countvectorizer__max_df': 0.5, 'countvectorizer__min_df': 4, 'countvectorizer__ngram_range': (1, 2), \n",
    "                       'countvectorizer__stop_words': 'english', 'countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', \n",
    "                       'kerasdensennwrapper__n_neuron': [40], 'kerasdensennwrapper__rate': 0.3}\n",
    "modelEightBestParms = {'kerascnnwrapper__embeddingdim': 20, 'kerascnnwrapper__filtersizes': [2, 3, 4], \n",
    "                       'kerascnnwrapper__numfilters': 3, 'kerascnnwrapper__vocabsize': 1500}\n",
    "modelNineBestParms = {'featureunion__pipeline__countvectorizer__ngram_range': (1, 1), \n",
    "                      'featureunion__pipeline__countvectorizer__stop_words': 'english', \n",
    "                      'featureunion__pipeline__countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'logisticregression__C': 0.1}\n",
    "modelTenBestParms = {'featureunion__pipeline__countvectorizer__ngram_range': (1, 1), \n",
    "                     'featureunion__pipeline__countvectorizer__stop_words': 'english', \n",
    "                     'featureunion__pipeline__countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', \n",
    "                     'xgbclassifier__learning_rate': 0.1, 'xgbclassifier__max_depth': 3, 'xgbclassifier__n_estimators': 50}\n",
    "\n",
    "\n",
    "\n",
    "### Evaluate model performance with tuned parameters ###\n",
    "opts = {'labelList': topicLabels}\n",
    "finalResults = pd.DataFrame(columns=['TestAcc', 'TestF1Macro']) # To store all results\n",
    "\n",
    "acc, f1macro = modelOne(X_train, y_train, X_test, y_test, tunedParms=modelOneBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelTwo(X_train, y_train, X_test, y_test, tunedParms=modelTwoBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelThree(X_train, y_train, X_test, y_test, tunedParms=modelThreeBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelFour(X_train, y_train, X_test, y_test, tunedParms=modelFourBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelFive(X_train, y_train, X_test, y_test, tunedParms=modelFiveBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelSix(X_train, y_train, X_test, y_test, tunedParms=modelSixBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelSeven(X_train, y_train, X_test, y_test, tunedParms=modelSevenBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelEight(X_train, y_train, X_test, y_test, tunedParms=modelEightBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelNine(X_train_withEmoji, y_train, X_test_withEmoji, y_test, tunedParms=modelNineBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelTen(X_train_withEmoji, y_train, X_test_withEmoji, y_test, tunedParms=modelTenBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "display(finalResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the best model for live prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \tModel 5\t =====\n",
      "Test accuracy\t0.582\n",
      "F1 micro\t0.582\n",
      "F1 macro\t0.520\n",
      "Confusion matrix\n",
      " [[202   6  21  29  15  44   9   7]\n",
      " [  1  10   0   2   0   0   0   0]\n",
      " [ 15   1  24   3   8   4   4   0]\n",
      " [ 10   0   4  34   5   4   4   2]\n",
      " [  3   0   1  10  38   5   1   0]\n",
      " [ 11   1   1   3   2  13   2   2]\n",
      " [  0   0   3   1   3   0  21   0]\n",
      " [  2   0   1   1   0   0   0   7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5816666666666667, 0.5200451580110859)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFive(X_train, y_train, X_test, y_test, tunedParms=modelFiveBestParms, saveModel=os.path.join(outFolder, 'BestModel_Topics.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Problem 3</u>  --  Predicting sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate X and y for topic\n",
    "y_train = labelled_tweets_Y_train.sentiment\n",
    "y_test = labelled_tweets_Y_test.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \tModel 1\t =====\n",
      "Test accuracy\t0.593\n",
      "F1 micro\t0.593\n",
      "F1 macro\t0.558\n",
      "Confusion matrix\n",
      " [[128  55  15]\n",
      " [ 90 189  57]\n",
      " [ 10  17  39]]\n",
      "===== \tModel 2\t =====\n",
      "Test accuracy\t0.632\n",
      "F1 micro\t0.632\n",
      "F1 macro\t0.576\n",
      "Confusion matrix\n",
      " [[114  79   5]\n",
      " [ 78 238  20]\n",
      " [  5  34  27]]\n",
      "===== \tModel 3\t =====\n",
      "Test accuracy\t0.632\n",
      "F1 micro\t0.632\n",
      "F1 macro\t0.465\n",
      "Confusion matrix\n",
      " [[ 83 115   0]\n",
      " [ 44 290   2]\n",
      " [  2  58   6]]\n",
      "===== \tModel 4\t =====\n",
      "Test accuracy\t0.657\n",
      "F1 micro\t0.657\n",
      "F1 macro\t0.610\n",
      "Confusion matrix\n",
      " [[127  57  14]\n",
      " [ 63 228  45]\n",
      " [  7  20  39]]\n",
      "===== \tModel 5\t =====\n",
      "Test accuracy\t0.615\n",
      "F1 micro\t0.615\n",
      "F1 macro\t0.583\n",
      "Confusion matrix\n",
      " [[127  55  16]\n",
      " [ 82 199  55]\n",
      " [  6  17  43]]\n",
      "===== \tModel 6\t =====\n",
      "Test accuracy\t0.678\n",
      "F1 micro\t0.678\n",
      "F1 macro\t0.559\n",
      "Confusion matrix\n",
      " [[ 86 109   3]\n",
      " [ 29 305   2]\n",
      " [  5  45  16]]\n",
      "===== \tModel 7\t =====\n",
      "Test accuracy\t0.675\n",
      "F1 micro\t0.675\n",
      "F1 macro\t0.574\n",
      "Confusion matrix\n",
      " [[ 99  95   4]\n",
      " [ 40 287   9]\n",
      " [  7  40  19]]\n",
      "===== \tModel 8\t =====\n",
      "Test accuracy\t0.615\n",
      "F1 micro\t0.615\n",
      "F1 macro\t0.494\n",
      "Confusion matrix\n",
      " [[ 81 116   1]\n",
      " [ 51 275  10]\n",
      " [  4  49  13]]\n",
      "===== \tModel 9\t =====\n",
      "Test accuracy\t0.557\n",
      "F1 micro\t0.557\n",
      "F1 macro\t0.521\n",
      "Confusion matrix\n",
      " [[124  47  27]\n",
      " [ 76 170  90]\n",
      " [ 12  14  40]]\n",
      "===== \tModel 10\t =====\n",
      "Test accuracy\t0.640\n",
      "F1 micro\t0.640\n",
      "F1 macro\t0.460\n",
      "Confusion matrix\n",
      " [[ 81 117   0]\n",
      " [ 29 298   9]\n",
      " [  5  56   5]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TestAcc</th>\n",
       "      <th>TestF1Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.593333</td>\n",
       "      <td>0.558261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.631667</td>\n",
       "      <td>0.575903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.631667</td>\n",
       "      <td>0.465238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.610012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.582825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.678333</td>\n",
       "      <td>0.558664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.573531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.494227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.556667</td>\n",
       "      <td>0.521090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.460370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TestAcc  TestF1Macro\n",
       "0  0.593333  0.558261   \n",
       "1  0.631667  0.575903   \n",
       "2  0.631667  0.465238   \n",
       "3  0.656667  0.610012   \n",
       "4  0.615000  0.582825   \n",
       "5  0.678333  0.558664   \n",
       "6  0.675000  0.573531   \n",
       "7  0.615000  0.494227   \n",
       "8  0.556667  0.521090   \n",
       "9  0.640000  0.460370   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Hyperparameter tuning using 5-fold cross validation ###\n",
    "tuning = False\n",
    "if tuning:\n",
    "    modelOneBestParms = modelOne(X_train, y_train, X_test, y_test)\n",
    "    print('modelOneBestParms = {}\\n\\n'.format(modelOneBestParms))\n",
    "\n",
    "    modelTwoBestParms = modelTwo(X_train, y_train, X_test, y_test)\n",
    "    print('modelTwoBestParms = {}\\n\\n'.format(modelTwoBestParms))\n",
    "\n",
    "    modelThreeBestParms = modelThree(X_train, y_train, X_test, y_test)\n",
    "    print('modelThreeBestParms = {}\\n\\n'.format(modelThreeBestParms))\n",
    "\n",
    "    modelFourBestParms = modelFour(X_train, y_train, X_test, y_test)\n",
    "    print('modelFourBestParms = {}\\n\\n'.format(modelFourBestParms))\n",
    "\n",
    "    modelFiveBestParms = modelFive(X_train, y_train, X_test, y_test)\n",
    "    print('modelFiveBestParms = {}\\n\\n'.format(modelFiveBestParms))\n",
    "\n",
    "    modelSixBestParms = modelSix(X_train, y_train, X_test, y_test)\n",
    "    print('modelSixBestParms = {}\\n\\n'.format(modelSixBestParms))\n",
    "    \n",
    "    modelSevenBestParms = modelSeven(X_train, y_train, X_test, y_test)\n",
    "    print('modelSevenBestParms = {}\\n\\n'.format(modelSevenBestParms))\n",
    "\n",
    "    modelEightBestParms = modelEight(X_train, y_train, X_test, y_test)\n",
    "    print('modelEightBestParms = {}\\n\\n'.format(modelEightBestParms))\n",
    "    \n",
    "    modelNineBestParms = modelNine(X_train_withEmoji, y_train, X_test_withEmoji, y_test)\n",
    "    print('modelNineBestParms = {}\\n\\n'.format(modelNineBestParms))\n",
    "\n",
    "    modelTenBestParms = modelTen(X_train_withEmoji, y_train, X_test_withEmoji, y_test)\n",
    "    print('modelTenBestParms = {}\\n\\n'.format(modelTenBestParms))\n",
    "\n",
    "\n",
    "\n",
    "### Use tuned hyperparameter ###\n",
    "modelOneBestParms = {'logisticregression__C': 0.1}\n",
    "modelTwoBestParms = {'countvectorizer__max_df': 0.5, 'countvectorizer__min_df': 5, 'countvectorizer__ngram_range': (1, 1), \n",
    "                     'countvectorizer__stop_words': 'english', 'countvectorizer__token_pattern': '\\\\b[^\\\\d\\\\W]+\\\\b', \n",
    "                     'multinomialnb__alpha': 1}\n",
    "modelThreeBestParms = {'multinomialnb__alpha': 0.2, 'tfidfvectorizer__max_df': 0.5, \n",
    "                       'tfidfvectorizer__min_df': 5, 'tfidfvectorizer__ngram_range': (1, 1), \n",
    "                       'tfidfvectorizer__stop_words': 'english', 'tfidfvectorizer__token_pattern': '\\\\b[^\\\\d\\\\W]+\\\\b'}\n",
    "modelFourBestParms = {'countvectorizer__max_df': 0.5, 'countvectorizer__min_df': 3, 'countvectorizer__ngram_range': (1, 2), \n",
    "                      'countvectorizer__stop_words': 'english', \n",
    "                      'countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'logisticregression__C': 0.3}\n",
    "modelFiveBestParms = {'logisticregression__C': 0.9, 'tfidfvectorizer__max_df': 0.5, 'tfidfvectorizer__min_df': 4, \n",
    "                      'tfidfvectorizer__ngram_range': (1, 2), \n",
    "                      'tfidfvectorizer__stop_words': 'english', 'tfidfvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b'}\n",
    "modelSixBestParms = {'countvectorizer__max_df': 0.5, 'countvectorizer__min_df': 3, 'countvectorizer__ngram_range': (1, 2), \n",
    "                     'countvectorizer__stop_words': 'english', 'countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', \n",
    "                     'xgbclassifier__learning_rate': 0.1, 'xgbclassifier__max_depth': 3, 'xgbclassifier__n_estimators': 200}\n",
    "modelSevenBestParms = {'countvectorizer__max_df': 0.5, 'countvectorizer__min_df': 4, 'countvectorizer__ngram_range': (1, 2), \n",
    "                       'countvectorizer__stop_words': 'english', 'countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', \n",
    "                       'kerasdensennwrapper__n_neuron': [40], 'kerasdensennwrapper__rate': 0.3}\n",
    "modelEightBestParms = {'kerascnnwrapper__embeddingdim': 20, 'kerascnnwrapper__filtersizes': [2, 3, 4], \n",
    "                       'kerascnnwrapper__numfilters': 3, 'kerascnnwrapper__vocabsize': 1500}\n",
    "modelNineBestParms = {'featureunion__pipeline__countvectorizer__ngram_range': (1, 2), 'featureunion__pipeline__countvectorizer__stop_words': 'english', \n",
    "                      'featureunion__pipeline__countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'logisticregression__C': 0.7}\n",
    "modelTenBestParms = {'featureunion__pipeline__countvectorizer__ngram_range': (1, 1), 'featureunion__pipeline__countvectorizer__stop_words': 'english', \n",
    "                     'featureunion__pipeline__countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'xgbclassifier__learning_rate': 0.05, \n",
    "                     'xgbclassifier__max_depth': 3, 'xgbclassifier__n_estimators': 50}\n",
    "\n",
    "\n",
    "\n",
    "### Evaluate model performance with tuned parameters ###\n",
    "opts = {'labelList': sentimentLabels}\n",
    "finalResults = pd.DataFrame(columns=['TestAcc', 'TestF1Macro']) # To store all results\n",
    "\n",
    "acc, f1macro = modelOne(X_train, y_train, X_test, y_test, tunedParms=modelOneBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelTwo(X_train, y_train, X_test, y_test, tunedParms=modelTwoBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelThree(X_train, y_train, X_test, y_test, tunedParms=modelThreeBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelFour(X_train, y_train, X_test, y_test, tunedParms=modelFourBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelFive(X_train, y_train, X_test, y_test, tunedParms=modelFiveBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelSix(X_train, y_train, X_test, y_test, tunedParms=modelSixBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelSeven(X_train, y_train, X_test, y_test, tunedParms=modelSevenBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelEight(X_train, y_train, X_test, y_test, tunedParms=modelEightBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelNine(X_train_withEmoji, y_train, X_test_withEmoji, y_test, tunedParms=modelNineBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "acc, f1macro = modelTen(X_train_withEmoji, y_train, X_test_withEmoji, y_test, tunedParms=modelTenBestParms, **opts)\n",
    "finalResults = finalResults.append({'TestAcc': acc, 'TestF1Macro': f1macro}, ignore_index=True)\n",
    "\n",
    "display(finalResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the best model for live prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \tModel 4\t =====\n",
      "Test accuracy\t0.657\n",
      "F1 micro\t0.657\n",
      "F1 macro\t0.610\n",
      "Confusion matrix\n",
      " [[127  57  14]\n",
      " [ 63 228  45]\n",
      " [  7  20  39]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6566666666666666, 0.6100120621064423)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFour(X_train, y_train, X_test, y_test, tunedParms=modelFourBestParms, saveModel=os.path.join(outFolder, 'BestModel_Sentiment.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
